{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:07:27.272118Z",
     "start_time": "2021-12-14T14:07:26.812948Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import gdown\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import ceil\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:07:28.524016Z",
     "start_time": "2021-12-14T14:07:27.939891Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "module_path = os.path.abspath(os.path.join('../src'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from training import *\n",
    "from utils import User, VisualSimilarityHandler, get_decaying_learning_rates, load_embeddings_and_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Load pre-trained image embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:07:30.256403Z",
     "start_time": "2021-12-14T14:07:30.251392Z"
    }
   },
   "outputs": [],
   "source": [
    "resnet_embedding_url = 'https://drive.google.com/uc?id=1fIplLP0Oyuilv-VgkbgAUzON7fKc2NUX'\n",
    "inventory_url = 'https://drive.google.com/uc?id=1Eskmof-qehDlp0-aiw60dP0dFVs6f2Fa'\n",
    "purchases_url = 'https://drive.google.com/uc?id=1tSTFCOj5WhQQFNPQsknfGF9pG1Cud7f4'\n",
    "clusters_url = 'https://drive.google.com/uc?id=19eHFoWexE-iqvpQZHDimAZqTNLdqVGOg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:07:40.710653Z",
     "start_time": "2021-12-14T14:07:31.352477Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1fIplLP0Oyuilv-VgkbgAUzON7fKc2NUX\n",
      "To: C:\\Users\\victor_accete\\CuratorNet\\data\\resnet_embeddings.npy\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 272M/272M [00:14<00:00, 19.2MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1Eskmof-qehDlp0-aiw60dP0dFVs6f2Fa\n",
      "To: C:\\Users\\victor_accete\\CuratorNet\\data\\inventory.csv\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 550k/550k [00:00<00:00, 1.20MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1tSTFCOj5WhQQFNPQsknfGF9pG1Cud7f4\n",
      "To: C:\\Users\\victor_accete\\CuratorNet\\data\\purchases.csv\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 429k/429k [00:00<00:00, 1.12MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=19eHFoWexE-iqvpQZHDimAZqTNLdqVGOg\n",
      "To: C:\\Users\\victor_accete\\CuratorNet\\data\\clusters.json\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 530k/530k [00:00<00:00, 1.10MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'../data/clusters.json'"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = '../data/'\n",
    "try:\n",
    "    os.mkdir(data_path)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "gdown.download(resnet_embedding_url, data_path + 'resnet_embeddings.npy')\n",
    "gdown.download(inventory_url, data_path + 'inventory.csv')\n",
    "gdown.download(purchases_url, data_path + 'purchases.csv')\n",
    "gdown.download(clusters_url, data_path + 'clusters.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:07:44.332397Z",
     "start_time": "2021-12-14T14:07:43.384570Z"
    }
   },
   "outputs": [],
   "source": [
    "resnet50 = load_embeddings_and_ids(data_path + 'resnet_embeddings.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Concatenate embeddings + z-score normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:07:45.531944Z",
     "start_time": "2021-12-14T14:07:45.525646Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_list = [resnet50,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:14:34.235846Z",
     "start_time": "2021-12-14T14:14:34.230592Z"
    }
   },
   "outputs": [],
   "source": [
    "artwork_ids_set = set()\n",
    "for embedding in embedding_list:\n",
    "    artwork_ids_set.update(embedding['index2id'].values())\n",
    "artwork_ids = list(artwork_ids_set)\n",
    "artwork_id2index = {_id:i for i,_id in enumerate(artwork_ids)}\n",
    "n_artworks = len(artwork_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:14:40.690816Z",
     "start_time": "2021-12-14T14:14:40.682244Z"
    }
   },
   "outputs": [],
   "source": [
    "featmat_list = [tmp['featmat'] for tmp in embedding_list]\n",
    "id2index_list = [tmp['id2index'] for tmp in embedding_list]\n",
    "concat_featmat = resnet50['featmat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:14:41.580901Z",
     "start_time": "2021-12-14T14:14:41.294202Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "concat_featmat = StandardScaler().fit_transform(concat_featmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Load clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:15:15.517052Z",
     "start_time": "2021-12-14T14:15:15.504697Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(data_path + 'clusters.json') as f:\n",
    "    artworkId2clusterId = json.load(f)\n",
    "cluster_ids = np.full((n_artworks,), -1, dtype=int)\n",
    "for k, v in artworkId2clusterId.items():\n",
    "    cluster_ids[artwork_id2index[k]] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:15:16.820725Z",
     "start_time": "2021-12-14T14:15:16.815191Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 99, (13297,))"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_ids.min(), cluster_ids.max(), cluster_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:15:17.930169Z",
     "start_time": "2021-12-14T14:15:17.924664Z"
    }
   },
   "outputs": [],
   "source": [
    "n_clusters = len(set(cluster_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:15:18.629018Z",
     "start_time": "2021-12-14T14:15:18.622361Z"
    }
   },
   "outputs": [],
   "source": [
    "clusterId2artworkIndexes = [[] for _ in range(n_clusters)]\n",
    "for i, cluster_id in enumerate(cluster_ids):\n",
    "    clusterId2artworkIndexes[cluster_id].append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Load PCA200 embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:15:20.104260Z",
     "start_time": "2021-12-14T14:15:20.076690Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:15:28.028077Z",
     "start_time": "2021-12-14T14:15:20.696323Z"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2, svd_solver='full')\n",
    "pca200_embeddings = pca.fit_transform(concat_featmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Load transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:15:30.646358Z",
     "start_time": "2021-12-14T14:15:30.629669Z"
    }
   },
   "outputs": [],
   "source": [
    "sales_df = pd.read_csv(data_path + 'purchases.csv')\n",
    "artworks_df = pd.read_csv(data_path + 'inventory.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:15:31.377301Z",
     "start_time": "2021-12-14T14:15:31.362815Z"
    }
   },
   "outputs": [],
   "source": [
    "artist2index = {a:i for i, a in enumerate(artworks_df.artist_id_hash.unique())}\n",
    "index2artist = {i:a for i, a in enumerate(artworks_df.artist_id_hash.unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:15:32.038823Z",
     "start_time": "2021-12-14T14:15:32.029653Z"
    }
   },
   "outputs": [],
   "source": [
    "artist_ids = np.full((n_artworks,), -1, dtype=int)\n",
    "failed = []\n",
    "for _artworkId, _artistId in zip(artworks_df.artwork_id_hash, artworks_df.artist_id_hash):\n",
    "    i = artwork_id2index[_artworkId]\n",
    "    artist_ids[i] = artist2index[_artistId]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:15:32.866842Z",
     "start_time": "2021-12-14T14:15:32.858013Z"
    }
   },
   "outputs": [],
   "source": [
    "artistId2artworkIndexes = dict()\n",
    "for i, _artistId in enumerate(artist_ids):\n",
    "    if _artistId == -1:\n",
    "        continue\n",
    "    try:\n",
    "        artistId2artworkIndexes[_artistId].append(i)\n",
    "    except KeyError:\n",
    "        artistId2artworkIndexes[_artistId] = [i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect transactions per user (making sure we hide the last nonfirst purchase basket per user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create list of users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:15:34.751070Z",
     "start_time": "2021-12-14T14:15:34.682699Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2919"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_ids = sales_df.user_id_hash.unique()\n",
    "user_id2index = { _id:i for i,_id in enumerate(user_ids) }\n",
    "users = [User(uid) for uid in user_ids]\n",
    "n_users = len(user_ids)\n",
    "n_users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### collect and sanity check transactions per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:15:35.851031Z",
     "start_time": "2021-12-14T14:15:35.841346Z"
    }
   },
   "outputs": [],
   "source": [
    "sorted_sales_df = sales_df.sort_values('purchase_timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:15:36.659336Z",
     "start_time": "2021-12-14T14:15:36.576375Z"
    }
   },
   "outputs": [],
   "source": [
    "# clear structures to prevent possible duplicate elements\n",
    "for user in users:\n",
    "    user.clear()\n",
    "\n",
    "# collect transactions per user sorted by timestamp\n",
    "for uid, a_ids, t in zip(sorted_sales_df.user_id_hash,\n",
    "                       sorted_sales_df.purchased_artwork_ids_hash,\n",
    "                       sorted_sales_df.purchase_timestamp):\n",
    "    for aid in ast.literal_eval(a_ids):\n",
    "        users[user_id2index[uid]].append_transaction(aid, t, artwork_id2index, artist_ids, cluster_ids)\n",
    "        assert users[user_id2index[uid]]._uid == uid\n",
    "    \n",
    "# bin transctions with same timestamps into purchase baskets\n",
    "for user in users:\n",
    "    user.build_purchase_baskets()\n",
    "    user.sanity_check_purchase_baskets()\n",
    "    user.remove_last_nonfirst_purchase_basket(\n",
    "        artwork_id2index, artist_ids, cluster_ids)\n",
    "    user.sanity_check_purchase_baskets()\n",
    "    user.refresh_nonpurchased_cluster_ids(n_clusters)\n",
    "    user.refresh_cluster_ids()\n",
    "    user.refresh_artist_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:15:38.090100Z",
     "start_time": "2021-12-14T14:15:38.084509Z"
    }
   },
   "outputs": [],
   "source": [
    "_MOD = 402653189\n",
    "_BASE = 92821\n",
    "\n",
    "def hash_triple(profile, pi, ni):\n",
    "    h = 0\n",
    "    for x in profile:\n",
    "        h = ((h * _BASE) % _MOD + x) % _MOD\n",
    "    h = ((h * _BASE) % _MOD + pi) % _MOD\n",
    "    h = ((h * _BASE) % _MOD + ni) % _MOD\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:15:38.965920Z",
     "start_time": "2021-12-14T14:15:38.943352Z"
    }
   },
   "outputs": [],
   "source": [
    "def sanity_check_instance(instance,\n",
    "                          pos_in_profile=True,\n",
    "                          profile_set=None,\n",
    "                          pos_sharing_cluster_artist=None,\n",
    "                          clusters_set=None,\n",
    "                          artists_set=None,\n",
    "                          neg_notsharing_artist=None,\n",
    "                          neg_notsharing_artist_cluster=None,\n",
    "                         ):\n",
    "    profile, pi, ni, ui = instance\n",
    "    try:\n",
    "        assert 0 <= pi < n_artworks\n",
    "        assert 0 <= ni < n_artworks\n",
    "        assert pi != ni        \n",
    "        assert not vissimhandler.same(pi,ni)\n",
    "        if ui == -1: return\n",
    "        \n",
    "        assert 0 <= ui < n_users\n",
    "        user = users[ui]\n",
    "        \n",
    "        assert all(i in user.artwork_idxs_set for i in profile)\n",
    "        \n",
    "        user_profile = user.artwork_idxs_set if profile_set is None else profile_set\n",
    "        \n",
    "        if pos_in_profile is True:\n",
    "            assert pi in user_profile\n",
    "        elif pos_in_profile is False:\n",
    "            assert pi not in user_profile\n",
    "            \n",
    "        if pos_sharing_cluster_artist:\n",
    "            assert cluster_ids[pi] in clusters_set\n",
    "            assert artist_ids[pi] in artists_set\n",
    "            \n",
    "        assert ni not in user_profile\n",
    "        \n",
    "        if neg_notsharing_artist:\n",
    "            assert artist_ids[ni] not in artists_set\n",
    "        if neg_notsharing_artist_cluster:\n",
    "            assert artist_ids[ni] not in artists_set\n",
    "            assert cluster_ids[ni] not in clusters_set\n",
    "\n",
    "    except AssertionError:\n",
    "        print('profile = ', profile)\n",
    "        print('pi = ', pi)\n",
    "        print('ni = ', ni)\n",
    "        print('ui = ', ui)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:15:39.435990Z",
     "start_time": "2021-12-14T14:15:39.430097Z"
    }
   },
   "outputs": [],
   "source": [
    "def append_instance(container, instance, **kwargs):\n",
    "    global _hash_collisions\n",
    "    profile, pi, ni, ui = instance\n",
    "    \n",
    "    h = hash_triple(profile, pi, ni)\n",
    "    if h in used_hashes:\n",
    "        _hash_collisions += 1\n",
    "        return False\n",
    "    \n",
    "    if vissimhandler.same(pi, ni):\n",
    "        return False\n",
    "    \n",
    "    sanity_check_instance(instance, **kwargs)\n",
    "    container.append(instance)\n",
    "    used_hashes.add(h)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:15:39.952007Z",
     "start_time": "2021-12-14T14:15:39.946896Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_triple(t):\n",
    "    profile, pi, ni, ui = t\n",
    "    print ('profile = ', [artwork_ids[i] for i in profile])\n",
    "    print ('pi = ', artwork_ids[pi])\n",
    "    print ('ni = ', artwork_ids[ni])\n",
    "    print ('ui = ', user_ids[ui] if ui != -1 else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:15:40.459832Z",
     "start_time": "2021-12-14T14:15:40.454381Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_num_samples(sampler_func):\n",
    "    def wrapper(instances_container, n_samples):\n",
    "        len_before = len(instances_container)\n",
    "        sampler_func(instances_container, n_samples)\n",
    "        actual_samples = len(instances_container) - len_before\n",
    "        print('  target samples: %d' % n_samples)\n",
    "        print('  actual samples: %d' % actual_samples)\n",
    "        print('  delta: %d' % (n_samples - actual_samples))\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:15:41.004445Z",
     "start_time": "2021-12-14T14:15:40.999948Z"
    }
   },
   "outputs": [],
   "source": [
    "vissimhandler = VisualSimilarityHandler(cluster_ids, pca200_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:15:41.616755Z",
     "start_time": "2021-12-14T14:15:41.613059Z"
    }
   },
   "outputs": [],
   "source": [
    "vissimhandler.count = 0\n",
    "used_hashes = set()\n",
    "_hash_collisions = 0\n",
    "train_instances = []\n",
    "test_instances = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:15:42.198658Z",
     "start_time": "2021-12-14T14:15:42.192236Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1666667, 83333)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_STRATEGIES = 6\n",
    "TOTAL_SAMPLES__TRAIN = int(1e7 + 2)\n",
    "TOTAL_SAMPLES__TEST =  int(np.round(TOTAL_SAMPLES__TRAIN*.05))\n",
    "N_SAMPLES_PER_STRATEGY__TRAIN = int(TOTAL_SAMPLES__TRAIN / N_STRATEGIES)\n",
    "N_SAMPLES_PER_STRATEGY__TEST = int(TOTAL_SAMPLES__TEST / N_STRATEGIES)\n",
    "N_SAMPLES_PER_STRATEGY__TRAIN, N_SAMPLES_PER_STRATEGY__TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:15:42.916150Z",
     "start_time": "2021-12-14T14:15:42.909478Z"
    }
   },
   "outputs": [],
   "source": [
    "FINE_GRAINED_THRESHOLD = 0.6  \n",
    "VISUAL_CONFIDENCE_THRESHOLD = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original BPR strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) given profile, recommend profile (real users)\n",
    "Given a user's profile, all items in the profile should be ranked higher than items outside the profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:15:45.986888Z",
     "start_time": "2021-12-14T14:15:45.981503Z"
    }
   },
   "outputs": [],
   "source": [
    "def sample_artwork_index__outsideprofile(profile_set, pi):\n",
    "    while True:\n",
    "        if random.random() <= FINE_GRAINED_THRESHOLD:\n",
    "            ni = random.choice(clusterId2artworkIndexes[cluster_ids[pi]])\n",
    "        else:            \n",
    "            c = random.randint(0, n_clusters-1)\n",
    "            ni = random.choice(clusterId2artworkIndexes[c])\n",
    "        if ni not in profile_set:\n",
    "            return ni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:15:46.819330Z",
     "start_time": "2021-12-14T14:15:46.813703Z"
    }
   },
   "outputs": [],
   "source": [
    "@print_num_samples\n",
    "def generate_samples__rank_profile_above_nonprofile(instances_container, n_samples):\n",
    "    n_samples_per_user = ceil(n_samples / n_users)    \n",
    "    for ui, user in enumerate(users):\n",
    "        profile = user.artwork_idxs\n",
    "        profile_set = user.artwork_idxs_set\n",
    "        for _ in range(n_samples_per_user):\n",
    "            for __ in range(5):\n",
    "                pi = random.choice(profile)\n",
    "                ni = sample_artwork_index__outsideprofile(profile_set, pi)\n",
    "                if append_instance(instances_container, (profile, pi, ni, ui),\n",
    "                                   profile_set=profile_set):\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:16:01.869280Z",
     "start_time": "2021-12-14T14:15:47.485894Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling train instances ...\n",
      "  target samples: 1666667\n",
      "  actual samples: 1620257\n",
      "  delta: 46410\n",
      "sampling test instances ...\n",
      "  target samples: 83333\n",
      "  actual samples: 79514\n",
      "  delta: 3819\n",
      "1620257 79514\n",
      "hash_collisions =  1266153\n",
      "visual_collisions =  457\n"
     ]
    }
   ],
   "source": [
    "print('sampling train instances ...')\n",
    "generate_samples__rank_profile_above_nonprofile(train_instances, n_samples=N_SAMPLES_PER_STRATEGY__TRAIN)\n",
    "print('sampling test instances ...')\n",
    "generate_samples__rank_profile_above_nonprofile(test_instances, n_samples=N_SAMPLES_PER_STRATEGY__TEST)\n",
    "print(len(train_instances), len(test_instances))\n",
    "print('hash_collisions = ', _hash_collisions)\n",
    "print('visual_collisions = ', vissimhandler.count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) Given profile, recommend profile (fake 1-item profiles)\n",
    "Given a fake profile of a single item, such item should be ranked higher than any other item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:16:36.336835Z",
     "start_time": "2021-12-14T14:16:36.331689Z"
    }
   },
   "outputs": [],
   "source": [
    "def sample_artwork_index__nonidentical(pi):        \n",
    "    while True:\n",
    "        if random.random() <= FINE_GRAINED_THRESHOLD:\n",
    "            ni = random.choice(clusterId2artworkIndexes[cluster_ids[pi]])\n",
    "        else:            \n",
    "            c = random.randint(0, n_clusters-1)\n",
    "            ni = random.choice(clusterId2artworkIndexes[c])\n",
    "        if ni != pi:\n",
    "            return ni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:16:36.852110Z",
     "start_time": "2021-12-14T14:16:36.840598Z"
    }
   },
   "outputs": [],
   "source": [
    "@print_num_samples\n",
    "def generate_samples__rank_single_item_above_anything_else(instances_container, n_samples):\n",
    "    n_samples_per_item = ceil(n_samples / n_artworks)\n",
    "    for pi in range(n_artworks):\n",
    "        profile = (pi,)\n",
    "        n = n_samples_per_item\n",
    "        while n > 0:\n",
    "            ni = sample_artwork_index__nonidentical(pi)\n",
    "            if append_instance(instances_container, (profile, pi, ni, -1)):\n",
    "                n -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:16:46.318930Z",
     "start_time": "2021-12-14T14:16:37.416457Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling train instances ...\n",
      "  target samples: 1666667\n",
      "  actual samples: 1675422\n",
      "  delta: -8755\n",
      "sampling test instances ...\n",
      "  target samples: 83333\n",
      "  actual samples: 93079\n",
      "  delta: -9746\n",
      "3295679 172593\n",
      "hash_collisions =  2005563\n",
      "visual_collisions =  766\n"
     ]
    }
   ],
   "source": [
    "print('sampling train instances ...')\n",
    "generate_samples__rank_single_item_above_anything_else(\n",
    "    train_instances, n_samples=N_SAMPLES_PER_STRATEGY__TRAIN)\n",
    "print('sampling test instances ...')\n",
    "generate_samples__rank_single_item_above_anything_else(\n",
    "    test_instances, n_samples=N_SAMPLES_PER_STRATEGY__TEST)\n",
    "print(len(train_instances), len(test_instances))\n",
    "print('hash_collisions = ', _hash_collisions)\n",
    "print('visual_collisions = ', vissimhandler.count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain-specific strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3) Recommend visually similar items from favorite artists (real users)\n",
    "Given a user, any item outside the user's profile that shares artist and visual cluster with items in the user's profile should be ranked higher than any item from an artist and visual cluster not present in the user's profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:16:48.532432Z",
     "start_time": "2021-12-14T14:16:48.526624Z"
    }
   },
   "outputs": [],
   "source": [
    "def sample_artwork_index__outsideprofile__sharing_artist_cluster(profile_set, artists_list, clusters_set):\n",
    "    for _ in range(20): # try at most 20 times\n",
    "        # sharing artist\n",
    "        a = random.choice(artists_list)\n",
    "        i = random.choice(artistId2artworkIndexes[a])\n",
    "        # sharing cluster\n",
    "        if cluster_ids[i] not in clusters_set: continue\n",
    "        # oustide profile\n",
    "        if i in profile_set: continue\n",
    "        # done\n",
    "        return i\n",
    "    return None # failed to find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:16:49.784163Z",
     "start_time": "2021-12-14T14:16:49.779179Z"
    }
   },
   "outputs": [],
   "source": [
    "def sample_artwork_index__notsharing_artist_cluster(artists_set, nonused_clusters_list):\n",
    "    while True:\n",
    "        # not sharing cluster\n",
    "        c = random.choice(nonused_clusters_list)\n",
    "        i = random.choice(clusterId2artworkIndexes[c])\n",
    "        # not sharing artist\n",
    "        if artist_ids[i] not in artists_set:\n",
    "            return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:16:50.909423Z",
     "start_time": "2021-12-14T14:16:50.903193Z"
    }
   },
   "outputs": [],
   "source": [
    "@print_num_samples\n",
    "def generate_samples__rank_sharing_artist_cluster_above_notsharing_artist_cluster(instances_container, n_samples):\n",
    "    n_samples_per_user = ceil(n_samples / n_users)    \n",
    "    for ui, user in enumerate(users):\n",
    "        profile = user.artwork_idxs\n",
    "        profile_set = user.artwork_idxs_set\n",
    "        artists_list = user.artist_ids\n",
    "        artists_set = user.artist_ids_set\n",
    "        clusters_set = user.cluster_ids_set\n",
    "        nonused_clusters_list = user.nonp_cluster_ids\n",
    "        for _ in range(n_samples_per_user):\n",
    "            for __ in range(5):\n",
    "                pi = sample_artwork_index__outsideprofile__sharing_artist_cluster(profile_set, artists_list, clusters_set)\n",
    "                if pi is None: continue\n",
    "                ni = sample_artwork_index__notsharing_artist_cluster(artists_set, nonused_clusters_list)\n",
    "                if append_instance(instances_container, (profile, pi, ni, ui),\n",
    "                                   pos_in_profile=False,\n",
    "                                   pos_sharing_cluster_artist=True,\n",
    "                                   neg_notsharing_artist_cluster=True,\n",
    "                                   profile_set = profile_set,\n",
    "                                   clusters_set = clusters_set,\n",
    "                                   artists_set = artists_set,\n",
    "                                  ):\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:18:06.050593Z",
     "start_time": "2021-12-14T14:16:52.027505Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling train instances ...\n",
      "  target samples: 1666667\n",
      "  actual samples: 1254359\n",
      "  delta: 412308\n",
      "sampling test instances ...\n",
      "  target samples: 83333\n",
      "  actual samples: 63657\n",
      "  delta: 19676\n",
      "4550038 236250\n",
      "hash_collisions =  2035500\n",
      "visual_collisions =  766\n"
     ]
    }
   ],
   "source": [
    "print('sampling train instances ...')\n",
    "generate_samples__rank_sharing_artist_cluster_above_notsharing_artist_cluster(\n",
    "    train_instances, n_samples=N_SAMPLES_PER_STRATEGY__TRAIN)\n",
    "print('sampling test instances ...')\n",
    "generate_samples__rank_sharing_artist_cluster_above_notsharing_artist_cluster(\n",
    "    test_instances, n_samples=N_SAMPLES_PER_STRATEGY__TEST)\n",
    "print(len(train_instances), len(test_instances))\n",
    "print('hash_collisions = ', _hash_collisions)\n",
    "print('visual_collisions = ', vissimhandler.count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4) Recommend visual similar items from favorite artists (fake 1-item profiles)\n",
    "Given a fake profile of a single item, other items sharing same artist should be ranked higher than items from different artists as long as the PCA200 embedding agrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:18:11.304699Z",
     "start_time": "2021-12-14T14:18:11.294896Z"
    }
   },
   "outputs": [],
   "source": [
    "def sample_artwork_index__nonidentical_sharing_artist(i):\n",
    "    a = artist_ids[i]\n",
    "    assert a != -1\n",
    "    candidate_idxs = artistId2artworkIndexes[a]\n",
    "    assert len(candidate_idxs) >= 2\n",
    "    while True:\n",
    "        pi = random.choice(candidate_idxs) # sharing artist\n",
    "        if pi != i: # non-identical\n",
    "            return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:18:11.869792Z",
     "start_time": "2021-12-14T14:18:11.864174Z"
    }
   },
   "outputs": [],
   "source": [
    "def sample_artwork_index__notsharing_artist__visually_acceptable(i, pi):\n",
    "    for _ in range(20): # try at most 20 times\n",
    "        if random.random() <= FINE_GRAINED_THRESHOLD:\n",
    "            ni = random.choice(clusterId2artworkIndexes[cluster_ids[i]])\n",
    "        else:\n",
    "            c = random.randint(0, n_clusters-1)\n",
    "            ni = random.choice(clusterId2artworkIndexes[c])\n",
    "        if artist_ids[ni] == artist_ids[i]: # not sharing artist\n",
    "            continue        \n",
    "        if vissimhandler.validate_triple(i, pi, ni, margin=VISUAL_CONFIDENCE_THRESHOLD): # visually acceptable\n",
    "            return ni\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:18:12.465710Z",
     "start_time": "2021-12-14T14:18:12.461361Z"
    }
   },
   "outputs": [],
   "source": [
    "@print_num_samples\n",
    "def generate_samples__rank_sharing_artist_above_notsharing_artist__visuallyacceptable__single_item(\n",
    "        instances_container, n_samples):\n",
    "    \n",
    "    n_valid_items = sum(1 for i in range(n_artworks) if artist_ids[i] != -1 and\\\n",
    "                        len(artistId2artworkIndexes[artist_ids[i]]) >= 2)\n",
    "    n_samples_per_item = ceil(n_samples / n_valid_items)\n",
    "    \n",
    "    for i in range(n_artworks):\n",
    "        a = artist_ids[i]\n",
    "        if a == -1 or len(artistId2artworkIndexes[a]) < 2:\n",
    "            continue\n",
    "        profile = (i,)\n",
    "        for _ in range(n_samples_per_item):\n",
    "            for __ in range(5):\n",
    "                pi = sample_artwork_index__nonidentical_sharing_artist(i)\n",
    "                ni = sample_artwork_index__notsharing_artist__visually_acceptable(i, pi)\n",
    "                if ni is None:\n",
    "                    continue\n",
    "                if append_instance(instances_container, (profile, pi, ni, -1)):\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:19:08.192431Z",
     "start_time": "2021-12-14T14:18:13.164169Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling train instances ...\n",
      "  target samples: 1666667\n",
      "  actual samples: 1658962\n",
      "  delta: 7705\n",
      "sampling test instances ...\n",
      "  target samples: 83333\n",
      "  actual samples: 82937\n",
      "  delta: 396\n",
      "6209000 319187\n",
      "hash_collisions =  2077476\n",
      "visual_collisions =  766\n"
     ]
    }
   ],
   "source": [
    "print('sampling train instances ...')\n",
    "generate_samples__rank_sharing_artist_above_notsharing_artist__visuallyacceptable__single_item(\n",
    "    train_instances, n_samples=N_SAMPLES_PER_STRATEGY__TRAIN)\n",
    "print('sampling test instances ...')\n",
    "generate_samples__rank_sharing_artist_above_notsharing_artist__visuallyacceptable__single_item(\n",
    "    test_instances, n_samples=N_SAMPLES_PER_STRATEGY__TEST)\n",
    "print(len(train_instances), len(test_instances))\n",
    "print('hash_collisions = ', _hash_collisions)\n",
    "print('visual_collisions = ', vissimhandler.count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5) Predict next purchase basket\n",
    "Given all previous purchases, rank each  item of the next purchase basket higher than any item from a never purchased artist and cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:19:08.197908Z",
     "start_time": "2021-12-14T14:19:08.193412Z"
    }
   },
   "outputs": [],
   "source": [
    "@print_num_samples\n",
    "def generate_samples__given_past_rank_next(instances_container, n_samples):\n",
    "    \n",
    "    n_valid_users = sum(1 for user in users if len(user.baskets) >= 2) # at last 2 purchase baskets\n",
    "    n_samples_per_user = ceil(n_samples / n_valid_users)\n",
    "    \n",
    "    for ui, user in enumerate(users):\n",
    "        n = len(user.baskets)\n",
    "        if n <= 1:\n",
    "            continue\n",
    "        past_items = []        \n",
    "        n_samples_per_basket = ceil(n_samples_per_user / (n-1))\n",
    "        for bi in range(n-1):\n",
    "            cur_b = user.baskets[bi]\n",
    "            for j in range(cur_b[0], cur_b[0] + cur_b[1]):\n",
    "                past_items.append(user.artwork_idxs[j])\n",
    "            next_b  = user.baskets[bi+1]\n",
    "            profile = past_items.copy()\n",
    "            for _ in range(n_samples_per_basket):\n",
    "                for __ in range(5):\n",
    "                    pi = user.artwork_idxs[random.randint(next_b[0], next_b[0] + next_b[1] - 1)]\n",
    "                    ni = sample_artwork_index__notsharing_artist_cluster(user.artist_ids_set, user.nonp_cluster_ids)\n",
    "                    if append_instance(instances_container, (profile, pi, ni, ui),\n",
    "                                      neg_notsharing_artist_cluster=True,\n",
    "                                      artists_set=user.artist_ids_set,\n",
    "                                      clusters_set=user.cluster_ids_set,\n",
    "                                      ):\n",
    "                        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:19:20.651528Z",
     "start_time": "2021-12-14T14:19:08.198742Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling train instances ...\n",
      "  target samples: 1666667\n",
      "  actual samples: 1663185\n",
      "  delta: 3482\n",
      "sampling test instances ...\n",
      "  target samples: 83333\n",
      "  actual samples: 82882\n",
      "  delta: 451\n",
      "7872185 402069\n",
      "hash_collisions =  2516126\n",
      "visual_collisions =  766\n"
     ]
    }
   ],
   "source": [
    "print('sampling train instances ...')\n",
    "generate_samples__given_past_rank_next(\n",
    "    train_instances, n_samples=N_SAMPLES_PER_STRATEGY__TRAIN)\n",
    "print('sampling test instances ...')\n",
    "generate_samples__given_past_rank_next(\n",
    "    test_instances, n_samples=N_SAMPLES_PER_STRATEGY__TEST)\n",
    "print(len(train_instances), len(test_instances))\n",
    "print('hash_collisions = ', _hash_collisions)\n",
    "print('visual_collisions = ', vissimhandler.count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6) Predict hidden item in the k-th purchase basket given first k\n",
    "Given the first k purchase baskets of a user, hide one item in the k-th purchase basket, use the rest as profile and rank the hidden item higher than any item from a never purchased artist and cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:19:20.658176Z",
     "start_time": "2021-12-14T14:19:20.652848Z"
    }
   },
   "outputs": [],
   "source": [
    "@print_num_samples\n",
    "def generate_samples__hide_and_predict_one_from_last__first_k_purchase_baskets(instances_container, n_samples):\n",
    "    \n",
    "    n_valid_baskets_list = [sum(1 for b in user.baskets if b[1] >= 2) for user in users]\n",
    "    n_valid_users = sum(1 for x in n_valid_baskets_list if x > 0)\n",
    "    n_samples_per_user = ceil(n_samples / n_valid_users)\n",
    "    \n",
    "    for ui, (user, n_valid_baskets) in enumerate(zip(users, n_valid_baskets_list)):\n",
    "        if n_valid_baskets == 0:\n",
    "            continue\n",
    "        n_samples_per_basket = ceil(n_samples_per_user / n_valid_baskets)\n",
    "        u_artwork_idxs = user.artwork_idxs\n",
    "        purchased = []\n",
    "        for b in user.baskets:            \n",
    "            bs = b[0]\n",
    "            be = b[0] + b[1]\n",
    "            purchased.extend(u_artwork_idxs[j] for j in range(bs, be))\n",
    "            assert len(purchased) == be\n",
    "            if b[1] < 2:\n",
    "                continue            \n",
    "            n_samples_per_item = ceil(n_samples_per_basket / b[1])            \n",
    "            for i in range(bs, be):\n",
    "                profile = [purchased[j] for j in range(be) if j != i]\n",
    "                assert len(profile) == be - 1\n",
    "                assert len(profile) > 0\n",
    "                pi = purchased[i]\n",
    "                for _ in range(n_samples_per_item):\n",
    "                    for __ in range(5):\n",
    "                        ni = sample_artwork_index__notsharing_artist_cluster(user.artist_ids_set, user.nonp_cluster_ids)\n",
    "                        if append_instance(instances_container, (profile, pi, ni, ui),\n",
    "                                          neg_notsharing_artist_cluster=True,\n",
    "                                          artists_set=user.artist_ids_set,\n",
    "                                          clusters_set=user.cluster_ids_set,\n",
    "                                          ):\n",
    "                            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:19:30.766904Z",
     "start_time": "2021-12-14T14:19:20.659065Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling train instances ...\n",
      "  target samples: 1666667\n",
      "  actual samples: 1667541\n",
      "  delta: -874\n",
      "sampling test instances ...\n",
      "  target samples: 83333\n",
      "  actual samples: 84420\n",
      "  delta: -1087\n",
      "9539726 486489\n",
      "hash_collisions =  2694275\n",
      "visual_collisions =  766\n"
     ]
    }
   ],
   "source": [
    "print('sampling train instances ...')\n",
    "generate_samples__hide_and_predict_one_from_last__first_k_purchase_baskets(\n",
    "    train_instances, n_samples=N_SAMPLES_PER_STRATEGY__TRAIN)\n",
    "print('sampling test instances ...')\n",
    "generate_samples__hide_and_predict_one_from_last__first_k_purchase_baskets(\n",
    "    test_instances, n_samples=N_SAMPLES_PER_STRATEGY__TEST)\n",
    "print(len(train_instances), len(test_instances))\n",
    "print('hash_collisions = ', _hash_collisions)\n",
    "print('visual_collisions = ', vissimhandler.count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sort train and test instances by profile size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:19:39.227574Z",
     "start_time": "2021-12-14T14:19:30.767787Z"
    }
   },
   "outputs": [],
   "source": [
    "random.shuffle(train_instances)\n",
    "train_instances.sort(key=lambda x: len(x[0]))\n",
    "test_instances.sort(key=lambda x: len(x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CuratorNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:19:39.230227Z",
     "start_time": "2021-12-14T14:19:39.228477Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:20:08.002898Z",
     "start_time": "2021-12-14T14:19:48.881906Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_tuples =  9539726\n",
      "n_batches =  484\n"
     ]
    }
   ],
   "source": [
    "train_minibatches = generate_minibatches(train_instances, max_users_items_per_batch=5000*10*2)\n",
    "sanity_check_minibatches(train_minibatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:20:08.863993Z",
     "start_time": "2021-12-14T14:20:08.004293Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_tuples =  486489\n",
      "n_batches =  25\n"
     ]
    }
   ],
   "source": [
    "test_minibatches = generate_minibatches(test_instances, max_users_items_per_batch=5000*10*2)\n",
    "sanity_check_minibatches(test_minibatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:20:08.867973Z",
     "start_time": "2021-12-14T14:20:08.865016Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0001,\n",
       " 6e-05,\n",
       " 3.6e-05,\n",
       " 2.16e-05,\n",
       " 1.296e-05,\n",
       " 7.776e-06,\n",
       " 4.6656e-06,\n",
       " 2.79936e-06,\n",
       " 1.679616e-06,\n",
       " 1.0077696e-06]"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#learning_rates = get_decaying_learning_rates(1e-6, 1e-8, 0.6)\n",
    "learning_rates = get_decaying_learning_rates(1e-4, 1e-6, 0.6)\n",
    "learning_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:20:08.877889Z",
     "start_time": "2021-12-14T14:20:08.869141Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6, 0.1)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FINE_GRAINED_THRESHOLD, VISUAL_CONFIDENCE_THRESHOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:20:08.886656Z",
     "start_time": "2021-12-14T14:20:08.878710Z"
    }
   },
   "outputs": [],
   "source": [
    "MODEL_PATH = 'C:\\\\Users\\\\victor_accete\\\\CuratorNet\\\\experiments\\\\curatornet_10m\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:20:08.893779Z",
     "start_time": "2021-12-14T14:20:08.887444Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19711"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_train_batch_size = ceil(np.mean([b.shape[0] for b in train_minibatches['profile_indexes_batches']]))\n",
    "avg_train_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-14T14:24:01.249309Z",
     "start_time": "2021-12-14T14:20:08.894617Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rates =  [0.0001, 6e-05, 3.6e-05, 2.16e-05, 1.296e-05, 7.776e-06, 4.6656e-06, 2.79936e-06, 1.679616e-06, 1.0077696e-06]\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001D1322F3198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001D1322F3198>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001D1322F3198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001D1322F3198>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001D1322F30F0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001D1322F30F0>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001D1322F30F0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001D1322F30F0>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001D12E0BDD68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001D12E0BDD68>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001D12E0BDD68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001D12E0BDD68>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001D12E0BDD68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001D12E0BDD68>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001D12E0BDD68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001D12E0BDD68>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001D12E0BDD68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001D12E0BDD68>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001D12E0BDD68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001D12E0BDD68>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001D13317E4A8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001D13317E4A8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001D13317E4A8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001D13317E4A8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001D133023080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001D133023080>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001D133023080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001D133023080>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001D1322F3198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001D1322F3198>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001D1322F3198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001D1322F3198>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001D1322F3198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001D1322F3198>>: AttributeError: module 'gast' has no attribute 'Index'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001D1322F3198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001D1322F3198>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "Variables to be trained:\n",
      "\t <tf.Variable 'trainable_item_embedding/fc1/kernel:0' shape=(2048, 200) dtype=float32_ref>\n",
      "\t <tf.Variable 'trainable_item_embedding/fc1/bias:0' shape=(200,) dtype=float32_ref>\n",
      "\t <tf.Variable 'trainable_item_embedding/fc2/kernel:0' shape=(200, 200) dtype=float32_ref>\n",
      "\t <tf.Variable 'trainable_item_embedding/fc2/bias:0' shape=(200,) dtype=float32_ref>\n",
      "\t <tf.Variable 'user_hidden_1/kernel:0' shape=(400, 300) dtype=float32_ref>\n",
      "\t <tf.Variable 'user_hidden_1/bias:0' shape=(300,) dtype=float32_ref>\n",
      "\t <tf.Variable 'user_hidden_2/kernel:0' shape=(300, 300) dtype=float32_ref>\n",
      "\t <tf.Variable 'user_hidden_2/bias:0' shape=(300,) dtype=float32_ref>\n",
      "\t <tf.Variable 'user_vector/kernel:0' shape=(300, 200) dtype=float32_ref>\n",
      "\t <tf.Variable 'user_vector/bias:0' shape=(200,) dtype=float32_ref>\n",
      "\t <tf.Variable 'beta1_power:0' shape=() dtype=float32_ref>\n",
      "\t <tf.Variable 'beta2_power:0' shape=() dtype=float32_ref>\n",
      "\t <tf.Variable 'trainable_item_embedding/fc1/kernel/Adam:0' shape=(2048, 200) dtype=float32_ref>\n",
      "\t <tf.Variable 'trainable_item_embedding/fc1/kernel/Adam_1:0' shape=(2048, 200) dtype=float32_ref>\n",
      "\t <tf.Variable 'trainable_item_embedding/fc1/bias/Adam:0' shape=(200,) dtype=float32_ref>\n",
      "\t <tf.Variable 'trainable_item_embedding/fc1/bias/Adam_1:0' shape=(200,) dtype=float32_ref>\n",
      "\t <tf.Variable 'trainable_item_embedding/fc2/kernel/Adam:0' shape=(200, 200) dtype=float32_ref>\n",
      "\t <tf.Variable 'trainable_item_embedding/fc2/kernel/Adam_1:0' shape=(200, 200) dtype=float32_ref>\n",
      "\t <tf.Variable 'trainable_item_embedding/fc2/bias/Adam:0' shape=(200,) dtype=float32_ref>\n",
      "\t <tf.Variable 'trainable_item_embedding/fc2/bias/Adam_1:0' shape=(200,) dtype=float32_ref>\n",
      "\t <tf.Variable 'user_hidden_1/kernel/Adam:0' shape=(400, 300) dtype=float32_ref>\n",
      "\t <tf.Variable 'user_hidden_1/kernel/Adam_1:0' shape=(400, 300) dtype=float32_ref>\n",
      "\t <tf.Variable 'user_hidden_1/bias/Adam:0' shape=(300,) dtype=float32_ref>\n",
      "\t <tf.Variable 'user_hidden_1/bias/Adam_1:0' shape=(300,) dtype=float32_ref>\n",
      "\t <tf.Variable 'user_hidden_2/kernel/Adam:0' shape=(300, 300) dtype=float32_ref>\n",
      "\t <tf.Variable 'user_hidden_2/kernel/Adam_1:0' shape=(300, 300) dtype=float32_ref>\n",
      "\t <tf.Variable 'user_hidden_2/bias/Adam:0' shape=(300,) dtype=float32_ref>\n",
      "\t <tf.Variable 'user_hidden_2/bias/Adam_1:0' shape=(300,) dtype=float32_ref>\n",
      "\t <tf.Variable 'user_vector/kernel/Adam:0' shape=(300, 200) dtype=float32_ref>\n",
      "\t <tf.Variable 'user_vector/kernel/Adam_1:0' shape=(300, 200) dtype=float32_ref>\n",
      "\t <tf.Variable 'user_vector/bias/Adam:0' shape=(200,) dtype=float32_ref>\n",
      "\t <tf.Variable 'user_vector/bias/Adam_1:0' shape=(200,) dtype=float32_ref>\n",
      "no checkpoint found: initializing variables with random values\n",
      "Before training: test_accuracy = 0.479559\n",
      "Starting training ...\n",
      "EPOCH=1 => train_i=483, train_loss = 0.761225671250, test_accuracy = 0.7167007, epoch_secs = 72.44, total_secs = 72.44\n",
      "   ** improvement detected: model saved to path  C:\\Users\\victor_accete\\CuratorNet\\experiments\\curatornet_10m\\\n",
      "EPOCH=2 => train_i=483, train_loss = 0.834293418131, test_accuracy = 0.8488517, epoch_secs = 72.46, total_secs = 144.89\n",
      "   ** improvement detected: model saved to path  C:\\Users\\victor_accete\\CuratorNet\\experiments\\curatornet_10m\\\n",
      "EPOCH=3 => train_i=483, train_loss = 0.741054183986, test_accuracy = 0.8704986, epoch_secs = 76.88, total_secs = 221.78\n",
      "   ** improvement detected: model saved to path  C:\\Users\\victor_accete\\CuratorNet\\experiments\\curatornet_10m\\\n",
      "EPOCH=4 => train_i=483, train_loss = 0.629805358178, test_accuracy = 0.8808174, epoch_secs = 74.81, total_secs = 296.59\n",
      "   ** improvement detected: model saved to path  C:\\Users\\victor_accete\\CuratorNet\\experiments\\curatornet_10m\\\n",
      "EPOCH=5 => train_i=483, train_loss = 0.522934204447, test_accuracy = 0.9024479, epoch_secs = 74.97, total_secs = 371.55\n",
      "   ** improvement detected: model saved to path  C:\\Users\\victor_accete\\CuratorNet\\experiments\\curatornet_10m\\\n",
      "EPOCH=6 => train_i=483, train_loss = 0.431015605518, test_accuracy = 0.9105077, epoch_secs = 75.32, total_secs = 446.87\n",
      "   ** improvement detected: model saved to path  C:\\Users\\victor_accete\\CuratorNet\\experiments\\curatornet_10m\\\n",
      "EPOCH=7 => train_i=483, train_loss = 0.357181355218, test_accuracy = 0.9199427, epoch_secs = 76.19, total_secs = 523.06\n",
      "   ** improvement detected: model saved to path  C:\\Users\\victor_accete\\CuratorNet\\experiments\\curatornet_10m\\\n",
      "EPOCH=8 => train_i=483, train_loss = 0.299242963624, test_accuracy = 0.9287384, epoch_secs = 78.88, total_secs = 601.95\n",
      "   ** improvement detected: model saved to path  C:\\Users\\victor_accete\\CuratorNet\\experiments\\curatornet_10m\\\n",
      "EPOCH=9 => train_i=483, train_loss = 0.253638781783, test_accuracy = 0.9321444, epoch_secs = 76.81, total_secs = 678.76\n",
      "   ** improvement detected: model saved to path  C:\\Users\\victor_accete\\CuratorNet\\experiments\\curatornet_10m\\\n",
      "EPOCH=10 => train_i=483, train_loss = 0.218174303222, test_accuracy = 0.9307528, epoch_secs = 73.43, total_secs = 752.19\n",
      "EPOCH=11 => train_i=483, train_loss = 0.190765803346, test_accuracy = 0.9197063, epoch_secs = 74.07, total_secs = 826.26\n",
      "   *** 2 checks with no improvements -> using a smaller learning_rate = 0.00006000\n",
      "EPOCH=12 => train_i=483, train_loss = 0.171040048905, test_accuracy = 0.9387715, epoch_secs = 75.57, total_secs = 901.84\n",
      "   ** improvement detected: model saved to path  C:\\Users\\victor_accete\\CuratorNet\\experiments\\curatornet_10m\\\n",
      "EPOCH=13 => train_i=483, train_loss = 0.155845985602, test_accuracy = 0.9395033, epoch_secs = 73.23, total_secs = 975.07\n",
      "   ** improvement detected: model saved to path  C:\\Users\\victor_accete\\CuratorNet\\experiments\\curatornet_10m\\\n",
      "EPOCH=14 => train_i=483, train_loss = 0.145397975903, test_accuracy = 0.9375402, epoch_secs = 74.63, total_secs = 1049.70\n",
      "EPOCH=15 => train_i=483, train_loss = 0.138282366217, test_accuracy = 0.9425927, epoch_secs = 74.50, total_secs = 1124.20\n",
      "   ** improvement detected: model saved to path  C:\\Users\\victor_accete\\CuratorNet\\experiments\\curatornet_10m\\\n",
      "EPOCH=16 => train_i=483, train_loss = 0.131685168602, test_accuracy = 0.9530123, epoch_secs = 81.89, total_secs = 1206.09\n",
      "   ** improvement detected: model saved to path  C:\\Users\\victor_accete\\CuratorNet\\experiments\\curatornet_10m\\\n",
      "EPOCH=17 => train_i=483, train_loss = 0.124772020391, test_accuracy = 0.9554070, epoch_secs = 79.91, total_secs = 1286.00\n",
      "   ** improvement detected: model saved to path  C:\\Users\\victor_accete\\CuratorNet\\experiments\\curatornet_10m\\\n",
      "EPOCH=18 => train_i=483, train_loss = 0.117857747146, test_accuracy = 0.9047810, epoch_secs = 75.63, total_secs = 1361.63\n",
      "EPOCH=19 => train_i=483, train_loss = 0.111738735826, test_accuracy = 0.9617258, epoch_secs = 76.41, total_secs = 1438.04\n",
      "   ** improvement detected: model saved to path  C:\\Users\\victor_accete\\CuratorNet\\experiments\\curatornet_10m\\\n",
      "EPOCH=20 => train_i=483, train_loss = 0.105930653906, test_accuracy = 0.9631564, epoch_secs = 76.75, total_secs = 1514.79\n",
      "   ** improvement detected: model saved to path  C:\\Users\\victor_accete\\CuratorNet\\experiments\\curatornet_10m\\\n",
      "EPOCH=21 => train_i=483, train_loss = 0.102188769358, test_accuracy = 0.9442433, epoch_secs = 78.98, total_secs = 1593.77\n",
      "EPOCH=22 => train_i=483, train_loss = 0.099089067961, test_accuracy = 0.9604698, epoch_secs = 82.10, total_secs = 1675.87\n",
      "   *** 2 checks with no improvements -> using a smaller learning_rate = 0.00003600\n",
      "EPOCH=23 => train_i=483, train_loss = 0.095849379960, test_accuracy = 0.9714012, epoch_secs = 77.88, total_secs = 1753.75\n",
      "   ** improvement detected: model saved to path  C:\\Users\\victor_accete\\CuratorNet\\experiments\\curatornet_10m\\\n",
      "EPOCH=24 => train_i=483, train_loss = 0.092303846079, test_accuracy = 0.9754958, epoch_secs = 79.87, total_secs = 1833.62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ** improvement detected: model saved to path  C:\\Users\\victor_accete\\CuratorNet\\experiments\\curatornet_10m\\\n",
      "EPOCH=25 => train_i=483, train_loss = 0.089436928161, test_accuracy = 0.9742543, epoch_secs = 84.49, total_secs = 1918.11\n",
      "EPOCH=26 => train_i=483, train_loss = 0.087101977281, test_accuracy = 0.9794548, epoch_secs = 77.33, total_secs = 1995.45\n",
      "   ** improvement detected: model saved to path  C:\\Users\\victor_accete\\CuratorNet\\experiments\\curatornet_10m\\\n",
      "EPOCH=27 => train_i=483, train_loss = 0.084906470465, test_accuracy = 0.9808012, epoch_secs = 76.16, total_secs = 2071.61\n",
      "   ** improvement detected: model saved to path  C:\\Users\\victor_accete\\CuratorNet\\experiments\\curatornet_10m\\\n",
      "EPOCH=28 => train_i=483, train_loss = 0.082606723646, test_accuracy = 0.9784229, epoch_secs = 78.40, total_secs = 2150.00\n",
      "EPOCH=29 => train_i=483, train_loss = 0.080427347046, test_accuracy = 0.9844765, epoch_secs = 82.05, total_secs = 2232.05\n",
      "   ** improvement detected: model saved to path  C:\\Users\\victor_accete\\CuratorNet\\experiments\\curatornet_10m\\\n",
      "EPOCH=30 => train_i=483, train_loss = 0.078451998600, test_accuracy = 0.9858044, epoch_secs = 76.02, total_secs = 2308.07\n",
      "   ** improvement detected: model saved to path  C:\\Users\\victor_accete\\CuratorNet\\experiments\\curatornet_10m\\\n",
      "====== TIMEOUT ======\n"
     ]
    }
   ],
   "source": [
    "train_network(\n",
    "    train_minibatches, test_minibatches,\n",
    "    len(train_instances), len(test_instances),\n",
    "    batch_size=avg_train_batch_size,\n",
    "    pretrained_embeddings=concat_featmat,\n",
    "    user_layer_units=[300,300,200],\n",
    "    item_layer_units=[200,200],\n",
    "    profile_pooling_mode='AVG+MAX',\n",
    "    model_path = MODEL_PATH,\n",
    "    epochs=30,\n",
    "    early_stopping_checks=2,\n",
    "    weight_decay=.0001,\n",
    "    learning_rates=learning_rates,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CuratorNetKernel",
   "language": "python",
   "name": "curatornetkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
